# Evidence: S05

## Segment
- ID: S05
- Deliverable(s): D09
- Date: 2026-02-12

## Code Changes
- Files changed:
  - `src/eval/__init__.py`
  - `src/eval/storage.py`
  - `src/eval/runner.py`
  - `tools/eval/agenthub_eval.py`
  - `data/evals/results.json`
  - `docs/eval/EVAL_FRAMEWORK.md`
  - `tests/eval/fixtures/three-capability-agent.yaml`
  - `tests/eval/test_eval_runner.py`
  - `tests/eval/test_eval_cli.py`
  - `src/api/app.py`
  - `tests/integration/test_registry_api.py`
  - `Makefile`
- Summary:
  - Implemented D09 eval framework core with deterministic Tier 1 contract-compliance runner.
  - Added sandbox-style execution (ephemeral tempdir, isolated run context, resource-limit metadata, trace capture).
  - Added structured eval result storage in `data/evals/results.json` with metrics (pass/fail, accuracy, cost, latency).
  - Added CLI integration via `agenthub eval` shim (`tools/eval/agenthub_eval.py`) and Makefile `eval` target.
  - Added Tier 1 fixture suite covering 3 capability categories (reasoning, transformation, action).
  - Integrated eval summaries into registry API responses (`GET /v1/agents/{id}`, versions endpoints) for UI consumption.
  - Added automated tests for runner behavior, CLI execution, and API eval-summary visibility.

## Test Commands Executed
```bash
pytest tests/eval -q
pytest tests/integration -q
pytest tests/capability_search -q
pytest tests/manifest -q
pytest tests/segment_s03 -q
python3 tools/eval/agenthub_eval.py eval --manifest tests/eval/fixtures/three-capability-agent.yaml --agent-id @eval:demo
make eval
pytest -q
```

## Test Results
- Passed:
  - `pytest tests/eval -q` -> `3 passed`
  - `pytest tests/integration -q` -> `9 passed`
  - `pytest tests/capability_search -q` -> `15 passed`
  - `pytest tests/manifest -q` -> `9 passed`
  - `pytest tests/segment_s03 -q` -> `8 passed`
  - Full suite `pytest -q` -> `44 passed`
  - CLI eval runs completed successfully with deterministic seed and structured JSON output.
- Failed:
  - None
- Skipped:
  - None

## Verification Evidence
- API contract validation:
  - Integration tests verify eval summaries are returned in registry detail response for UI rendering.
  - Version endpoints include eval summary data for version-level visibility.
- Security validation:
  - Eval runner records isolated sandbox metadata and executes checks in ephemeral directories.
  - Tier 1 contract checks enforce side-effect/idempotency consistency for action capabilities.
- Cost/latency validation:
  - Eval metrics include per-run `cost_usd` and `latency_ms`.
  - Example CLI run output included:
    - `suite_id=tier1-contract-v1`
    - `status=passed`
    - `accuracy=1.0`
    - `latency_ms` reported
    - `cost_usd` reported
- Logs/screenshots:
  - CLI JSON output and pytest command logs captured in terminal execution history.

## Known Risks
- Current sandbox is a local isolated tempdir model; production-grade microVM enforcement (Firecracker/E2B) is planned for later segments.
- Tier 2 and Tier 3 evals are not implemented yet; D09 scope currently delivers Tier 1 core as foundation.

## Next Segment Handoff
- Recommended next segment: S06
- Required context for next segment:
  - Eval storage schema and sample records: `data/evals/results.json`
  - Runner and trace model: `src/eval/runner.py`
  - CLI invocation contract: `tools/eval/agenthub_eval.py`
  - API eval summary integration points in `src/api/app.py`

## Mandatory Confirmation
- [x] `docs/agenthub-master-plan-v1.2.docx` updated
- [x] `docs/DELIVERABLE_LOG.md` updated
